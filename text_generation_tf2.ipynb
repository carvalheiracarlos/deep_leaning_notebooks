{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carvalheiracarlos/deep_leaning_notebooks/blob/main/text_generation_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7c0a2c3d",
      "metadata": {
        "id": "7c0a2c3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bde89fb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde89fb1",
        "outputId": "4015cc49-a06e-4f4a-f81a-28f6dddfdccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file ='shakespeare.txt'\n",
        "if os.path.exists(path_to_file) is False:\n",
        "    path_to_file =tf.keras.utils.get_file(\n",
        "        'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "37bcbf48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bcbf48",
        "outputId": "807a6c96-3401-48bc-a497-f07fc34ddd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4af63cb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af63cb4",
        "outputId": "9d135997-b320-427b-919b-6b1bd6880455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cc9c0d5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc9c0d5f",
        "outputId": "0a63b277-3a4c-4b51-cd29-50308797d62f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "17db3f80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17db3f80",
        "outputId": "85ad0c8b-501c-4686-e151-b76942287943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Convert the strings to a numerical representation.\n",
        "# The tf.keras.layers.StringLookup layer can convert each character into a numeric ID.\n",
        "# It just needs the text to be split into tokens first.\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b9bf8226",
      "metadata": {
        "id": "b9bf8226"
      },
      "outputs": [],
      "source": [
        "# characters -> numbers (layer)\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "93875ee7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93875ee7",
        "outputId": "8a38bd34-9333-4b7e-91cb-9cb8f5b7496c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# It converts from tokens to character IDs:\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1cfcb82a",
      "metadata": {
        "id": "1cfcb82a"
      },
      "outputs": [],
      "source": [
        "# numbers -> ids (layer)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3de844b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3de844b7",
        "outputId": "0e7390c0-b56f-4269-d0ae-a8ee23f3254e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# convert numbers -> characters\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0ebc65f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebc65f6",
        "outputId": "9d2fe397-5b79-44f0-9af3-9442bc4d7402"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# join characters back into text (layer)\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7369c23b",
      "metadata": {
        "id": "7369c23b"
      },
      "outputs": [],
      "source": [
        "# convert numbers into text\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d1f55e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1f55e80",
        "outputId": "9bb49a11-d00c-427b-bd3e-21d6ddda559e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# text -> stream of indices (numbers)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0f71ee33",
      "metadata": {
        "id": "0f71ee33"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b02cf50b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b02cf50b",
        "outputId": "f920e68d-6a59-4788-8820-077162a72869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2d610c5e",
      "metadata": {
        "id": "2d610c5e"
      },
      "outputs": [],
      "source": [
        "# size of the textual sequences to be used\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d51ca34a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d51ca34a",
        "outputId": "80cf674e-f296-436a-886f-438d3968f73e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "379d5e16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "379d5e16",
        "outputId": "80ccbee1-22c1-4886-ac8c-d302ec1d4451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7339140f",
      "metadata": {
        "id": "7339140f"
      },
      "outputs": [],
      "source": [
        "# For training you'll need a dataset of (input, label) pairs.\n",
        "# Where input and label are sequences.\n",
        "# At each time step the input is the current character and the label is the next character.\n",
        "# Here's a function that takes a sequence as input, duplicates, and shifts it\n",
        "# to align the input and label for each timestep:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b2e75c6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2e75c6a",
        "outputId": "f44dc869-f614-4cbd-acc7-520508fe73d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# example\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b674d281",
      "metadata": {
        "id": "b674d281"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "281f7cfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "281f7cfa",
        "outputId": "f4bd857f-5ce8-4b5b-feb3-00d5d04be980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e38cfa81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e38cfa81",
        "outputId": "fa8d6d7f-5fee-4190-a49a-c82fab82a1cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3543afe4",
      "metadata": {
        "id": "3543afe4"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "649f87a2",
      "metadata": {
        "id": "649f87a2"
      },
      "outputs": [],
      "source": [
        "# building the model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "193e2f7b",
      "metadata": {
        "id": "193e2f7b"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4dfe32b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dfe32b1",
        "outputId": "76eda34a-d21a-4e78-cc2a-d0bd38ae4f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# trying the model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a74c531d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a74c531d",
        "outputId": "12e30d26-9a55-4c7d-c16c-09d1fbcbe55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "159a837e",
      "metadata": {
        "id": "159a837e"
      },
      "outputs": [],
      "source": [
        "# To get actual predictions from the model you need to sample from the output distribution,\n",
        "#to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "940e9460",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "940e9460",
        "outputId": "2e250a43-339c-4645-bbb9-667a204397fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 48, 53, 12,  8,  4, 60, 24, 15, 60, 53, 17, 44, 28,  6, 49, 12,\n",
              "       10, 39, 39, 47, 61, 42,  9, 52,  9, 60, 19, 62,  5, 11, 31, 14,  0,\n",
              "       30, 65, 46, 46, 11, 15, 16, 10,  5, 21, 53,  8, 57, 45, 49, 32, 13,\n",
              "       12, 41, 49, 62, 37, 26, 55, 23, 43, 56,  3,  2, 27, 38, 33, 27, 17,\n",
              "       40, 32,  7,  5, 12, 26, 53, 17, 21,  5, 28, 41, 18, 60, 33, 15, 43,\n",
              "       45, 39,  2, 22, 15, 43, 36, 39, 42, 53,  2, 63, 27,  8, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# example\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a2e3da85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e3da85",
        "outputId": "b6a99693-d69e-4576-9b67-9b02a1a3e42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' behavior\\nFit for her turn, well read in poetry\\nAnd other books, good ones, I warrant ye.\\n\\nHORTENSIO'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"ein;-$uKBunDeO'j;3ZZhvc.m.uFw&:RA[UNK]Qzgg:BC3&Hn-rfjS?;bjwXMpJdq! NYTNDaS,&;MnDH&ObEuTBdfZ IBdWZcn xN-M\"\n"
          ]
        }
      ],
      "source": [
        "# Decode these to see the text predicted by this untrained model:\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6f66a77b",
      "metadata": {
        "id": "6f66a77b"
      },
      "outputs": [],
      "source": [
        "# cost function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e912f171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e912f171",
        "outputId": "ae4290cc-6041-419d-f3cf-143a150f1764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188131, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# cost (untrained)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8e59084e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e59084e",
        "outputId": "62f08195-3625-49d4-de1c-9c6096376d11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.8995"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ba48600f",
      "metadata": {
        "id": "ba48600f"
      },
      "outputs": [],
      "source": [
        "# model compilation\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1272ac42",
      "metadata": {
        "id": "1272ac42"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2d67753b",
      "metadata": {
        "id": "2d67753b"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1e9e1fb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e9e1fb6",
        "outputId": "d07b2137-fea6-4841-a561-e460bc9783d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 2.7076\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.9829\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7048\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.5434\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4439\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3759\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3226\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2779\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2365\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1971\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1565\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1149\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0711\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0249\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9766\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9260\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8742\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8218\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7716\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.7240\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "04baa17b",
      "metadata": {
        "id": "04baa17b"
      },
      "outputs": [],
      "source": [
        "# text generation model\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "09448bf8",
      "metadata": {
        "id": "09448bf8"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d49e5103",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d49e5103",
        "outputId": "b0db3304-c19c-46bc-e071-9387e7094287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The queen's good falcon: I have ta'en no other diancles,\n",
            "Draw town, will you advent destroying win.\n",
            "Neither, good fellows, if it be another,\n",
            "If ever he knew you, so please the kings.\n",
            "\n",
            "JULIET:\n",
            "Madam, we shall be so: even son\n",
            "Are calms not for that word of royal king,\n",
            "Can death in strokes: these hate two cup of the dead\n",
            "Millow should be accused for some sworn of heaven's\n",
            "Happiness was you ablined.\n",
            "\n",
            "PROSPERO:\n",
            "I shall let no such sighs.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Can you fellow?\n",
            "\n",
            "Elbow:\n",
            "Were it not, room to see a good bruther by him.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Pray, be it so.\n",
            "\n",
            "Servant:\n",
            "His name is Lady Bonquerio?\n",
            "O, Thursday speak, I see, sir. Mile, your one;\n",
            "For I have heard the ord hath done--\n",
            "Let me desprison? 't, this sad I\n",
            "not be rulleh, the ope? The duke is made\n",
            "So many a reseech with thousand o'er her\n",
            "confines faster: I'll too can be it as this.\n",
            "\n",
            "GLOUCESTER:\n",
            "The outrage is thy speech.\n",
            "\n",
            "QUEEN:\n",
            "To hop a prince is made by Advisau' ready.\n",
            "\n",
            "JULIET:\n",
            "What storm had quiet, nothing but him befail to have.\n",
            "\n",
            "RO \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4156694412231445\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c57ee649",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c57ee649",
        "outputId": "8e4240e4-e37e-4da4-9314-30a9c570ec28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nA sing look when I. but humbly in briefing harm\\nAnd kingly government to deck my body metal?\\nHappy the air a dozing of the contract, yea, ope:\\nA gracious riege or sweet and fair devotion,\\nYour myself let them play we all hap oft but them;\\nBut safe discover but subs it herself\\nUpon their base-gaited eyes of greeting\\nHanging.\\n\\nDUKE VINCENTIO:\\nIt is now mad I am going\\nTonetue in his power. For I have often\\nhouse our harms and unbusold hoodrymen steep'd.\\n\\nGLOUCESTER:\\nUpon me: there's a cold friar may not:\\nwherefore Camillo that have ta'tly call'dly in a bloody ignorance,\\nAnd I would forgue for never arrove her breath:\\nNever wither what I say, is from my heart:\\nThe trumpets on him had wet o' the earth\\nAnd leanness mere me swell in hooth the stain;\\nLest thou abive me boots of sovereignty,\\nLike sorrowers like a consul!\\n\\nBARNARDINE:\\nMy lord! how sweet a ring that briefs of me;\\nAnd when every gaps in hooth these?\\nRestur me my lords by him that drinksing her\\nsuch gentle less imagined, are nice \"\n",
            " b\"ROMEO:\\nI call usl'er.\\n\\nARIEL:\\nSpeak, what can you voice\\nWhich is none at your oath, as thou in me now,\\nThat weeps shall crossed to encounter nume, and came from hence,\\nConquerors was it made it return'd,\\nHe would early by yours, and thus may long;\\nand so are there, the singing be despite,\\n'Those dares and rotten toes, Dukes friar,\\nAnd wrepp'd in signation I will love thee\\nSo in concluit of healship did reciot.\\n\\nEXETER:\\nThe issue lightn point that ever we\\npass'd: some was too cordias, and they have accomp\\nTo sun, together with you, tender for steel and\\nfeldly at myself.\\n\\nLEONTES:\\nI have spoil'd\\nWe wedded against their loss, by silent;\\nhear me with you.\\n\\nFRANCISIO:\\nFollow me,\\nThat I myself have paid my stead, and I\\nDo think how see it.\\n\\nPROSPERO:\\nIf thou more quiet or nexed to\\nexpedition!\\n\\nFirst Soldier:\\nO my dear-lord, my lord, I'll quickly believe them by the\\ndoor.\\n\\nAUTOLINUUS:\\nHave said, my lord,\\nI have seen themselves: the objection of your graves\\nWe cannot be made a fouler than an adgesh;\"\n",
            " b\"ROMEO:\\nO go, I say!\\nEnclosion Camisle how to appect my honesty!\\n\\nPETRUCHIO:\\nSo please you, grantam.\\n\\nDUCHESS OF YORK:\\nI can tell you my brother say, I partle my lord!\\nIf both he never think it can dast out\\nTo surporriphed in more than all I sit,\\n'could think she was provided to our prattle and have veets,\\nTherewith I am credel, in sainted man,\\nFrom evermional gentleman I cannot grow.\\n\\nPETRUCHIO:\\nAnd thou shalt wear it.\\nREven from Lord Angelo, tell me not death's before thou'rt best.\\nThen say I sent thee here, thou! Taste our quarrel: from her,\\nAnd if he chide our lady's head: 'Conject you?\\n\\nBENVOLIO:\\nNo, your Vincentio slaughter you to't.\\n\\nFRIAR LAURENCE:\\nHold, here, a Jacky? is it so? By all things so young\\nRame it behones the beast: out of my father's life\\nFor lasters are disposed thus fornicate\\nto prove such soon and full of rubs, or stay.\\n\\nMENENIUS:\\nOn very sitterning most upundancles,\\nTo pause him in their galliase;\\nThe one being so bled dogs than ever think\\nTo die from me do pow and de\"\n",
            " b\"ROMEO:\\nI am in law, it can drop mercy: on me in rage\\n'This dare no need; who is it escaped?\\n\\nPRINCE EDWARD:\\nNay, go thy way home withdlaw togs, I beg forth.\\n\\nCAMILLO:\\nGood my lord, your fellow-spright on mine.\\n\\nVOLUMNIA:\\nO, welcome home!\\n\\nCURTIS:\\nLook, one worseful spectacle!\\nWhy stand said my father is at their escaper:\\nReso the heavens do right one hang out the advantage.\\n\\nBAPTISTA:\\nIf I persuade your grace I have it,\\nAnd most awsured to eace the day send for thee to't\\nWith that we'l hardly inder to't.\\n\\nPERDITA:\\nSo, sir.\\n\\nESCALUS:\\nBe collected:\\nIt could not stay.\\n\\nSEBASTIAN:\\nAy, and do't.\\n\\nFirst Citizen:\\nIn what ill doves upon myself?\\n\\nCLARENCE:\\nMethinks I am sure: it is not sit so far\\nfrom hands: boint beasts we hear their hearts.\\nYour knee may know you have comfort me,\\nAnd you and you were wellow had I bury\\nhis countrymen: wherefore hail! have you e'er thine!\\n\\nQUEEN ELIZABETH:\\nShall I seem rose again?\\n\\nSecond Conspirator:\\nHow may I say?\\n\\nKATHARINA:\\nMadam,\\nI am so barden! I will take my l\"\n",
            " b\"ROMEO:\\nThen I, poor monstrous, a restored soil\\nFor's variant faithful marriage was a dishonour.\\n\\nGLOUEET:\\nNo, made a dance, and Thursday excellence\\ndiscover all, be loath to pause away your hands,\\nConsidering the places of your sire of it.\\n\\nCAMILLO:\\nNay, if I fall in time:\\nWhat is the matters, each of the body will encounter\\nMuch correction to be in restory.\\n\\nCAPULET:\\nAll things I have: but when an urged\\nWere blood of death borongs for justice?\\n\\nFLORIZEL:\\nFetch me too more.\\n\\nLORD HENRUOFE:\\nThen will I in, but earna that you have been i' the people\\nAnd let the eye discolden race their lords,\\nTo ine work to those that did blood it as sacred hands,\\nThrough all unportain'd in the stage,\\nSpeaking in place Within by.\\n\\nFRIAR LAURENCE:\\nHold, danger, I will go and brow it.\\n\\nQUEEN ELIZABETH:\\nA brother died, and will, none rouge:\\nNow shall be mender'd with our keepers stands,\\nI crevilver of them and how he harsh to\\nprove a sovereign that dost thou do me does and marks.\\nLook here and long all humbly I d\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.3736038208007812\n"
          ]
        }
      ],
      "source": [
        "# batch text generation\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7bde3e10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bde3e10",
        "outputId": "d0e1c28a-0e6d-468a-a8bc-87b852e2f799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7ff2fa3f0810>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# save/load model\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "baf6fafc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baf6fafc",
        "outputId": "c92037cf-8533-43ae-cbc5-dca6b87bedbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Is not ta'en too much best besides spare of that join,\n",
            "And none but fulty bloods, the Volsces have \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34645c5b",
      "metadata": {
        "id": "34645c5b"
      },
      "source": [
        "# customized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebaf086f",
      "metadata": {
        "id": "ebaf086f"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a53babc",
      "metadata": {
        "id": "4a53babc"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c903f188",
      "metadata": {
        "id": "c903f188"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307119b0",
      "metadata": {
        "id": "307119b0"
      },
      "outputs": [],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14d325d",
      "metadata": {
        "id": "b14d325d"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d539497f",
      "metadata": {
        "id": "d539497f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}