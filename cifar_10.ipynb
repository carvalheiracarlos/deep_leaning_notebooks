{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjJCo/mVqVsJa2v7tg21iK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carvalheiracarlos/deep_leaning_notebooks/blob/main/cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dn9CgTNIVVj1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXnDmXR7RDr2",
        "outputId": "133f09de-3cd1-4dc0-a2c1-c5f247c69ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "e_MaQE6vVo-w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train Sets Shapes.....X_train: {X_train.shape}')\n",
        "print(f'Train Sets Shapes.....y_train: {y_train.shape}')\n",
        "print(f'Tests Sets Shapes.....X_test: {X_test.shape}')\n",
        "print(f'Tests Sets Shapes.....y_test: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2r2aNq2WE9h",
        "outputId": "4ce618e7-70c7-47a1-9392-ee4a3603183a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Sets Shapes.....X_train: (50000, 32, 32, 3)\n",
            "Train Sets Shapes.....y_train: (50000, 1)\n",
            "Tests Sets Shapes.....X_test: (10000, 32, 32, 3)\n",
            "Tests Sets Shapes.....y_test: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[22])\n",
        "print(y_train[22])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "wnuu5lkPWL0o",
        "outputId": "fc4a4a25-5682-449e-d82c-da7870fc0218"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale images to the [0, 1] range\n",
        "X_train = X_train.astype(\"float32\") / 255\n",
        "X_test = X_test.astype(\"float32\") / 255\n",
        "\n",
        "\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "kapRG19JoGeg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_images, validation_labels = X_train[:5000], y_train[:5000]\n",
        "train_images, train_labels = X_train[5000:], y_train[5000:]"
      ],
      "metadata": {
        "id": "jTTA50igRMad"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Referencies https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98"
      ],
      "metadata": {
        "id": "_i8GWUqDSYUn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(image, label):\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    image = tf.image.resize(image, (32,32))\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "L-7zwU73QyDg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yvCJrfHRp8o",
        "outputId": "a70ffdf8-bc87-4eea-9846-64cc49ecc3a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
        "print(\"Training data size:\", train_ds_size)\n",
        "print(\"Test data size:\", test_ds_size)\n",
        "print(\"Validation data size:\", validation_ds_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnWJ-wI1Q7M4",
        "outputId": "dbbce08b-8fc5-45c4-fe2c-ea68e187314c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 45000\n",
            "Test data size: 10000\n",
            "Validation data size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (train_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=10, drop_remainder=True))\n",
        "test_ds = (test_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=10, drop_remainder=True))\n",
        "validation_ds = (validation_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=10, drop_remainder=True))"
      ],
      "metadata": {
        "id": "K_6RWQYFR9Qf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightAlexNet:\n",
        "  @staticmethod\n",
        "  def build(n_channels, \n",
        "            rows, \n",
        "            cols, \n",
        "            n_classes, \n",
        "            activation='relu', \n",
        "            weights_path=None):\n",
        "    model = keras.models.Sequential()\n",
        "    \n",
        "    input_Shape = (rows, cols, n_channels)\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(filters=96, \n",
        "                                  kernel_size=(5,5),\n",
        "                                  strides=(2,2),\n",
        "                                  activation=activation, \n",
        "                                  input_shape=input_Shape))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "    \n",
        "    \n",
        "    model.add(keras.layers.Conv2D(filters=256, \n",
        "                                  kernel_size=(5,5),\n",
        "                                  strides=(1,1),\n",
        "                                  activation=activation, \n",
        "                                  input_shape=input_Shape,\n",
        "                                  padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "    \n",
        "    \n",
        "    model.add(keras.layers.Conv2D(filters=384, \n",
        "                                  kernel_size=(3,3),\n",
        "                                  strides=(1,1),\n",
        "                                  activation=activation, \n",
        "                                  input_shape=input_Shape,\n",
        "                                  padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    \n",
        "    model.add(keras.layers.Conv2D(filters=384, \n",
        "                                  kernel_size=(3,3),\n",
        "                                  strides=(1,1),\n",
        "                                  activation=activation, \n",
        "                                  input_shape=input_Shape,\n",
        "                                  padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    \n",
        "    model.add(keras.layers.Conv2D(filters=384, \n",
        "                                  kernel_size=(3,3),\n",
        "                                  strides=(1,1),\n",
        "                                  activation=activation, \n",
        "                                  input_shape=input_Shape,\n",
        "                                  padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    \n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(50, activation=activation))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    \n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(50, activation=activation))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    # if a weights path is supplied (inicating that the model was\n",
        "    # pre-trained), then load the weights\n",
        "    if weights_path is not None:\n",
        "      model.load_weights(weights_path)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "jJWgMrAZb4_q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1\n",
        "ini_learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "FTNgZU6PIWU_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0') as sess:\n",
        "    train_model = LightAlexNet.build(n_channels=3, \n",
        "                                     rows=32,\n",
        "                                     cols=32, \n",
        "                                     n_classes=10, \n",
        "                                     activation='relu', \n",
        "                                     weights_path=None)\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=ini_learning_rate, decay=ini_learning_rate / n_epochs)\n",
        "    train_model.compile(loss='categorical_crossentropy', \n",
        "                        optimizer=opt, \n",
        "                        metrics=[keras.metrics.categorical_accuracy])\n",
        "    train_model.summary()\n",
        "    try:\n",
        "        file_path = 'weights/LightAlexNet.h5'\n",
        "        check_point = keras.callbacks.ModelCheckpoint(file_path, \n",
        "                                                      monitor='accuracy', \n",
        "                                                      verbose=0, \n",
        "                                                      save_best_only=True,\n",
        "                                                      mode='max',\n",
        "                                                      save_weights_only=True)\n",
        "        #add tensorboard callbacks\n",
        "        tensorboard = keras.callbacks.TensorBoard('./logs/fit/')\n",
        "        callbacks_list = [check_point, tensorboard]\n",
        "        \n",
        "        fit_history = train_model.fit(x=X_train,\n",
        "                                      y=y_train,\n",
        "                                      epochs=n_epochs,\n",
        "                                      validation_split=0.1,\n",
        "                                      validation_freq=1,\n",
        "                                      callbacks=callbacks_list,\n",
        "                                      verbose=1)\n",
        "    \n",
        "   \n",
        "    except IOError:\n",
        "        print('Error while saving the Model weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Vw5CfuIYU4",
        "outputId": "b78bb886-391a-4cd6-e3af-82fbf68fb5ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_17 (Conv2D)          (None, 14, 14, 96)        7296      \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 14, 14, 96)       384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 6, 6, 96)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 6, 6, 256)         614656    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 2, 2, 384)         885120    \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 2, 2, 384)        1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 2, 2, 384)         1327488   \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 2, 2, 384)        1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 2, 2, 384)         1327488   \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 2, 2, 384)        1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                76850     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,247,974\n",
            "Trainable params: 4,244,966\n",
            "Non-trainable params: 3,008\n",
            "_________________________________________________________________\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 2.3086 - categorical_accuracy: 0.0976"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1407/1407 [==============================] - 32s 10ms/step - loss: 2.3086 - categorical_accuracy: 0.0976 - val_loss: 2.3027 - val_categorical_accuracy: 0.0976\n"
          ]
        }
      ]
    }
  ]
}